{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mat4py import loadmat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import resample\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"  # or if you want more than 1 GPU set it as \"0\", \"1\"\n",
    "import tensorflow as tf\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import sklearn\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization,Concatenate,concatenate, Input\n",
    "from tensorflow.keras.layers import Conv2D, Conv1D, MaxPooling1D\n",
    "#from tensorflow.keras.utils import np_utils\n",
    "#from tensorflow.keras.utils.vis_utils import model_to_dot\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import LSTM\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_string=['ucddb002','ucddb003','ucddb019','ucddb020','ucddb021','ucddb022','ucddb023','ucddb024',\n",
    "             ]#\\'ucddb008'\\,'ucddb011','ucddb013','ucddb014','ucddb010','ucddb012','ucddb014','ucddb015','ucddb017',\\,\\'ucddb025','ucddb026','ucddb027','ucddb028','ucddb005','ucddb006','ucddb007','ucddb009'\n",
    "\n",
    "\n",
    "valid_features=np.zeros((1,1408))\n",
    "test_features=np.zeros((1,1408))\n",
    "valid_labels=np.zeros((1,1))\n",
    "test_labels=np.zeros((1,1))\n",
    "for l in list_string:\n",
    "        \n",
    "        #displaying data in a good way\n",
    "        ecg_valid = loadmat('D:\\\\mahika\\\\notes\\\\sem6\\\\minor\\\\dataset\\\\selected\\\\'+l+'_ecg_valid.mat') \n",
    "        ecg_valid = np.array(ecg_valid['ecg_valid'])\n",
    "        ecg_valid_labels=loadmat('D:\\\\mahika\\\\notes\\\\sem6\\\\minor\\\\dataset\\\\selected\\\\'+l+'_valid_labels.mat')\n",
    "        ecg_valid_labels = np.array(ecg_valid_labels['class_valid'])\n",
    "        valid_features=np.append(valid_features,ecg_valid,axis=0)\n",
    "        valid_labels=np.append(valid_labels,ecg_valid_labels)\n",
    "       \n",
    "        \n",
    "        ecg_test = loadmat('D:\\\\mahika\\\\notes\\\\sem6\\\\minor\\\\dataset\\\\selected\\\\'+l+'_ecg_test.mat')\n",
    "        ecg_test = np.array(ecg_test['ecg_test'])\n",
    "        ecg_test_labels=loadmat('D:\\\\mahika\\\\notes\\\\sem6\\\\minor\\\\dataset\\\\selected\\\\'+l+'_test_labels.mat')\n",
    "        ecg_test_labels = np.array(ecg_test_labels['class_test'])\n",
    "        test_features=np.append(test_features,ecg_test,axis=0)\n",
    "        test_labels=np.append(test_labels,ecg_test_labels)\n",
    "        \n",
    "ecg_valid=valid_features[1:,:]\n",
    "#ecg_valid=np.expand_dims(ecg_valid, axis=2)\n",
    "valid_labels=valid_labels[1:]\n",
    "valid_labels = valid_labels.flatten()\n",
    "\n",
    "ecg_test=test_features[1:,:]\n",
    "#ecg_test=np.expand_dims(ecg_test, axis=2)\n",
    "test_labels=test_labels[1:]\n",
    "test_labels = test_labels.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features=np.zeros((1,1408))\n",
    "\n",
    "train_labels=np.zeros((1,1))\n",
    "\n",
    "for l in list_string:\n",
    "        ecg_train = loadmat('D:\\\\mahika\\\\notes\\\\sem6\\\\minor\\\\dataset\\\\selected\\\\'+l+'_ecg_train.mat')\n",
    "        ecg_train = np.array(ecg_train['ecg_train'])\n",
    "        ecg_train_labels=loadmat('D:\\\\mahika\\\\notes\\\\sem6\\\\minor\\\\dataset\\\\selected\\\\'+l+'_train_labels.mat')\n",
    "        ecg_train_labels = np.array(ecg_train_labels['class_train'])\n",
    "        train_features=np.append(train_features,ecg_train,axis=0)\n",
    "        train_labels=np.append(train_labels,ecg_train_labels)\n",
    "        \n",
    "ecg_train=train_features[1:,:]\n",
    "#ecg_train=np.expand_dims(ecg_train, axis=2)\n",
    "train_labels=train_labels[1:]\n",
    "train_labels = train_labels.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_train_mean=np.mean(ecg_train)\n",
    "ecg_train_std=np.std(ecg_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(ecg_train.shape[0]):\n",
    "    ecg_train[i,:]=(ecg_train[i,:]-ecg_train_mean)/ecg_train_std\n",
    "    \n",
    "for i in range(ecg_valid.shape[0]):\n",
    "    ecg_valid[i,:]=(ecg_valid[i,:]-ecg_train_mean)/ecg_train_std\n",
    "    \n",
    "for i in range(ecg_test.shape[0]):\n",
    "    ecg_test[i,:]=(ecg_test[i,:]-ecg_train_mean)/ecg_train_std\n",
    "\n",
    "ecg_train=np.expand_dims(ecg_train, axis=2)\n",
    "ecg_valid=np.expand_dims(ecg_valid, axis=2)\n",
    "ecg_test=np.expand_dims(ecg_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=train_labels\n",
    "y_valid=valid_labels\n",
    "y_test=test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorflow.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "y_train_num = y_train_encoder.fit_transform(y_train)\n",
    "y_train_wide = tensorflow.keras.utils.to_categorical(y_train_num, num_classes)\n",
    "\n",
    "y_valid_num = y_train_encoder.fit_transform(y_valid)\n",
    "y_valid_wide = tensorflow.keras.utils.to_categorical(y_valid_num, num_classes)\n",
    "\n",
    "y_test_num = y_train_encoder.fit_transform(y_test)\n",
    "y_test_wide = tensorflow.keras.utils.to_categorical(y_test_num, num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1D-CNN Sequential\n",
    "model_ecg = Sequential()\n",
    "model_ecg.add(BatchNormalization(input_shape=(1408,1)))\n",
    "model_ecg.add(Conv1D(3, kernel_size=(100),strides=2))\n",
    "model_ecg.add(Activation(\"relu\"))\n",
    "model_ecg.add(MaxPooling1D(pool_size=(2),strides=2))\n",
    "model_ecg.add(Conv1D(50, (10)))\n",
    "model_ecg.add(MaxPooling1D(pool_size=(2),strides=2))\n",
    "model_ecg.add(Activation(\"relu\"))\n",
    "model_ecg.add(Conv1D(30, (30)))\n",
    "model_ecg.add(MaxPooling1D(pool_size=(2)))\n",
    "model_ecg.add(Activation(\"relu\"))\n",
    "model_ecg.add(BatchNormalization())\n",
    "model_ecg.add(Flatten())\n",
    "model_ecg.add(Dropout(0.25))\n",
    "model_ecg.add(Dense(2, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation='softmax'))\n",
    "\n",
    "model_ecg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 1\n",
    "\n",
    "opt = tensorflow.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model_ecg.compile(loss='binary_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_weights_filepath_ecg = './best_weights_ecg_32layer.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_weights_filepath_ecg = './best_weights_ecg_32layer.hdf5'\n",
    "mcp_ecg = ModelCheckpoint(best_weights_filepath_ecg, monitor=\"val_accuracy\",\n",
    "                      save_best_only=True, save_weights_only=False)\n",
    "            \n",
    "history = model_ecg.fit(ecg_train, y_train_wide,\n",
    "         batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(ecg_valid, y_valid_wide),\n",
    "          callbacks=[mcp_ecg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "acc=history.history['accuracy']\n",
    "val_acc=history.history['val_accuracy']\n",
    "\n",
    "plt.figure(figsize=(6,12))\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "\n",
    "plt.xlabel('Epochs(100)')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(loss, 'green', label='Training Loss')\n",
    "plt.plot(val_loss, 'blue', label='Validation Loss')\n",
    "plt.xticks(range(0,epochs)[0::100])\n",
    "plt.title('Training and Validation Loss vs Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "\n",
    "plt.xlabel('Epochs(100)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(acc, 'green', label='Training Accuracy')\n",
    "plt.plot(val_acc, 'blue', label='Validation Accuracy')\n",
    "plt.xticks(range(0,epochs)[0::100])\n",
    "plt.title('Training and Validation Accuracy vs Epochs')\n",
    "plt.legend()\n",
    "plt.savefig(\"plots_perf.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ecg.load_weights('./best_weights_ecg_32layer.hdf5')\n",
    "y_pred = model_ecg.predict(ecg_test)\n",
    "predict_test=np.argmax(y_pred, axis=1)\n",
    "predict_test=predict_test.reshape(predict_test.shape[0],1)\n",
    "cm=confusion_matrix(y_test_num, predict_test)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true=y_test_num, y_pred=predict_test)\n",
    "\n",
    "# Print the confusion matrix using Matplotlib\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    " \n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "plt.title('Confusion Matrix', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import sqrt \n",
    "\n",
    "# Extract the true positive, true negative, false positive, and false negative values from the confusion matrix\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    " \n",
    "# Print the true positive, true negative, false positive, and false negative values\n",
    "print(\"True Positive (TP): \", tp)\n",
    "print(\"True Negative (TN): \", tn)\n",
    "print(\"False Positive (FP): \", fp)\n",
    "print(\"False Negative (FN): \", fn)\n",
    " \n",
    "# Calculate accuracy\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    " \n",
    "# Calculate precision\n",
    "precision = tp / (tp + fp)\n",
    " \n",
    "# Calculate sensitivity\n",
    "sen = tp / (tp + fn)\n",
    "\n",
    "#calculate specificity\n",
    "spc = tn / (tn + fp)\n",
    " \n",
    "# Calculate F1-score\n",
    "f1_score = 2 * (precision * sen) / (precision + sen)\n",
    "\n",
    "#calculate error rate \n",
    "err = (fp + fn) / (tp + tn + fp + fn)\n",
    "\n",
    "# #calculate positive prediction value\n",
    "# ppv = tp / (tp + fp)\n",
    "\n",
    "#calculate negative prediction value \n",
    "npv = tn / (tn + fn)\n",
    "\n",
    "#calculate false positive rate \n",
    "fpr = fp / (tp + fn)\n",
    "\n",
    "#calculate false negative rate \n",
    "fnr = fn / (tp + fp)\n",
    "\n",
    "#calculate false discovery rate\n",
    "fdr = fp / (fp + tp)\n",
    "\n",
    "#calculate matthew's correlation coefficient\n",
    "mcc = ((tp * tn) - (fp * fn)) / sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "#calculate true positive rate\n",
    "tpr = tp / (tp + fn)\n",
    " \n",
    "# Print the accuracy, precision, recall, and F1-score\n",
    "print(\"\\n\\nMetrics:\")\n",
    "print(\"Accuracy: \", round(accuracy, 2))\n",
    "print(\"Precision: \", round(precision, 2))\n",
    "print(\"sensitivity: \", round(sen, 2))\n",
    "print(\"specificity: \", round(spc,2))\n",
    "print(\"F1-score: \", round(f1_score, 2))\n",
    "print(\"Error Rate: \", round(err,2))\n",
    "print(\"NPV: \", round(npv,2))\n",
    "print(\"FPR: \", round(fpr,2))\n",
    "print(\"FNR: \", round(fnr,2))\n",
    "print(\"FDR: \", round(fdr,2))\n",
    "print(\"MCC: \", round(mcc,2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "print(type(model_ecg))\n",
    "\n",
    "def classify_output(output):\n",
    "    if np.any(output) == 1:\n",
    "        return \"Yes\"\n",
    "    else:\n",
    "        return \"No\"\n",
    "\n",
    "mat = scipy.io.loadmat(\"D:\\\\mahika\\\\notes\\\\sem6\\\\minor\\\\dataset\\\\selected\\\\ucddb002_ecg_test.mat\")\n",
    "print(mat.keys())\n",
    "x = mat['ecg_test']\n",
    "\n",
    "prediction = model_ecg.predict(x)\n",
    "print(prediction)\n",
    "classification = classify_output(prediction)\n",
    "print(classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 0.02\n",
    "y = 0.8696\n",
    "\n",
    "#create ROC curve\n",
    "plt.subplots(1, figsize=(5,5))\n",
    "plt.title('Receiver Operating Characteristic - LSTM')\n",
    "plt.plot(x, y)\n",
    "plt.plot([0, 1], ls=\"--\")\n",
    "plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "# Load .mat file\n",
    "mat = scipy.io.loadmat('D:\\\\mahika\\\\notes\\\\sem6\\\\minor\\\\dataset\\\\selected\\\\ucddb002_ecg_test.mat')\n",
    "\n",
    "# Get the input data from the .mat file\n",
    "print(mat.keys())\n",
    "x = mat['ecg_test']\n",
    "\n",
    "def classify_output(output):\n",
    "    if np.any(output) == 1:\n",
    "        return \"Yes\"\n",
    "    else:\n",
    "        return \"No\"\n",
    "\n",
    "# Make a prediction using the loaded model\n",
    "prediction = model_ecg.predict(x)\n",
    "classification = classify_output(prediction)\n",
    "print(classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
